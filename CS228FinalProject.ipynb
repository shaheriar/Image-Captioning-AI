{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 228 Final Project\n",
        "## Enhancing Image Captioning with Deep Learning Models\n",
        "### Saul Gonzalez - sgonz081\n",
        "### Shaheriar Malik - smali032\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "Image captioning is a difficult task that is one step above image classification since we are\n",
        "generating an actual text description of each image. So, deep learning would be an obvious choice in this case since generating text for a variable input image is a difficult task that would require a complex model.\n"
      ],
      "metadata": {
        "id": "jNU4I8Ai0yzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fgiadna8xoGq",
        "outputId": "66a29504-3a31-4935-d220-08930d0a5cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n",
        "import os\n",
        "import natsort\n",
        "import cv2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/My Drive/ImageCaptioningDataset/flickr30k_images.zip\" -d \"/content\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "anVSuIG2PNc5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('/content/drive/MyDrive/ImageCaptioningDataset/results.csv', sep='|', header = None)\n",
        "#raw_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "#raw_df.columns = raw_df.columns.str.replace(' ', '')\n",
        "#raw_df.loc[19999,'comment'] = 'A dog runs across the grass .'\n",
        "#raw_df.to_csv('out.csv',sep='|',index=False)\n",
        "#raw_df.iloc[[19999]]"
      ],
      "metadata": {
        "id": "rGJLib9AXtS3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read, resize and zero pad images. Returns image tensor [3, 256, 256]\n",
        "def readImage(path):\n",
        "    desired_size = 256 # 256 x 256\n",
        "    image = cv2.imread(path)\n",
        "    old_size = image.shape[:2]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(new_im)\n",
        "    return tensor\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    num = temp_df.shape[0]\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(num):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n",
        "        image = readImage(f\"/content/flickr30k_images/{temp_df.image_name[i]}\")\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.comment[i], 30)))\n",
        "\n",
        "#num = 5\n",
        "#display_images(raw_df.sample(num))"
      ],
      "metadata": {
        "id": "MfB4KsAgZTrB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X = np.zeros((158915,3,256,256))\n",
        "#y = list()\n",
        "#for i in range(len(raw_df)):\n",
        "#  image = readImage(f\"/content/drive/MyDrive/ImageCaptioningDataset/flickr30k_images/{raw_df.image_name[i]}\")\n",
        "#  X[i] = np.array(image)\n",
        "#  y.append(raw_df.comment[i])\n",
        "#  clear_output()\n",
        "#  print(i)\n"
      ],
      "metadata": {
        "id": "8_m_YqQbPrRZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary for Captions\n",
        "def build_vocab(captions,tokenizer,max_len):\n",
        "  counter = Counter()\n",
        "  current_max = max_len\n",
        "  for caption in captions:\n",
        "    sentence = caption.strip()\n",
        "    tokens = tokenizer(sentence)\n",
        "    if len(tokens) > current_max:\n",
        "      current_max = len(tokens)\n",
        "    counter.update(tokens)\n",
        "  sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  dictionary = OrderedDict(sorted_by_freq)\n",
        "  dictionary = dict(dictionary)\n",
        "  dictionary['<PAD>'] = len(dictionary)\n",
        "  dictionary = OrderedDict(dictionary)\n",
        "  return vocab(dictionary), current_max"
      ],
      "metadata": {
        "id": "E2vsl3IoFom4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_csv, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_len = -1\n",
        "\n",
        "        self.data_df = pd.read_csv(label_csv, sep='|',header=None)\n",
        "        self.data_df = self.data_df.dropna()\n",
        "        #self.data_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "        #self.data_df.columns = self.data_df.columns.str.replace(' ', '')\n",
        "        self.captions = self.data_df[1] # 0 is image_name, 1 is comment\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.data_df[0]#os.listdir(data_dir)  # Assumes images are directly under data_dir\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.vocab, self.target_len = build_vocab(self.captions.tolist(),self.tokenizer,self.target_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.fromarray(image)\n",
        "        #image = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[idx]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        # Vectorize the given caption using our dataset's vocabulary\n",
        "        caption = caption.strip()\n",
        "        caption = self.tokenizer(caption)\n",
        "        caption_vector = [self.vocab[word] for word in caption]\n",
        "\n",
        "        # Add padding to the vector if it needs it\n",
        "        if len(caption_vector) < self.target_len:\n",
        "          for i in range(self.target_len - len(caption_vector)):\n",
        "            caption_vector.append(self.vocab['<PAD>'])\n",
        "\n",
        "        # Return the processed image and any associated labels\n",
        "        return image, torch.tensor(caption_vector)\n"
      ],
      "metadata": {
        "id": "pKYERZ5AUtFM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "data_dir = '/content/flickr30k_images/'\n",
        "label_csv = '/content/drive/MyDrive/ImageCaptioningDataset/results.csv'\n",
        "dataset = CustomDataset(data_dir, label_csv, transform=transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vxtZgwO6e21r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea7424e-e166-4a1c-d00d-3566915730b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = raw_df[0]\n",
        "image_paths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZGiNPk2eyir",
        "outputId": "7b4b70b7-5951-445a-869f-a59aae4c2080"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000092795.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 37\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EfbFZUHSVJ16"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# i = 0\n",
        "# for img, file_name in dataloader:\n",
        "#   clear_output()\n",
        "#   #print(label[0])\n",
        "#   #print(raw_df['image_name'])\n",
        "#   plt.imshow(img[0].permute(1,2,0))\n",
        "#   print(list(raw_df.loc[raw_df['image_name'] == file_name[0]].comment)[0])\n",
        "#   i+=5\n",
        "#   plt.show()\n",
        "#   time.sleep(1)"
      ],
      "metadata": {
        "id": "8Om2xhbrV8Kf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neual Network (CNN)"
      ],
      "metadata": {
        "id": "NwlKzpy10_cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tXi1uiPd0_XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "bkHmM37T9822",
        "outputId": "847af584-615d-4fbb-ce82-8fd418a0fafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20217"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        \n",
        "        # Load the pretrained ResNet-101 model\n",
        "        self.resnet = models.wide_resnet50_2(weights=None)\n",
        "        self.resnet.fc = nn.Linear(2048, embed_size)\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.resnet(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        \n",
        "        embeddings = self.embedding(captions)\n",
        "        \n",
        "        combined = torch.cat((features, embeddings), dim=1)\n",
        "        \n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "        \n",
        "        outputs = self.fc(lstm_out)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6vCRizEKHyg0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256  # Size of the word embedding\n",
        "hidden_size = 512  # Size of the LSTM hidden state\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "v04TdT8KIShA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "def training():\n",
        "  for epoch in range(num_epochs):\n",
        "      i = 0\n",
        "      for images, captions in dataloader:\n",
        "          # Move images and captions to device (e.g., GPU)\n",
        "          images = images.to(device)\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # Reshape captions for loss calculation\n",
        "          targets = captions[:, :].reshape(-1)\n",
        "          \n",
        "          # Compute loss\n",
        "          loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print the loss\n",
        "          if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(dataloader), loss.item()))\n",
        "          i+=1\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)"
      ],
      "metadata": {
        "id": "v3eCeIOEIjTI",
        "outputId": "d47bac6d-2469-42ac-afb4-61940009eff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}